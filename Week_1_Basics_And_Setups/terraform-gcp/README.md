# Google Cloud Platform and Terraform

### Table of Contents
-  [Introduction to Google Cloud Platform](#introduction-to-google-cloud-platform)
    - [GCP setup](#gcp-setup)
    - [GCP access setup](#gcp-access-setup)
- [Terraform](#terraform)
    - [Terraform setup]()
    - [Terraform basics]()
    - [Create GCP resources with Terraform]()

# Introduction to Google Cloud Platform

Google Cloud Platform (GCP) is a cloud computing service offered by Google. This platform offers multiple services that are hosted on hardware provided by Google. Typical services provided are virtual machines (compute service), storage, application development and deployment, etc.

These services have pricing on them, usually depends on how much resource was used, but some services are provided with *Free Tier*, usually in the form of usage up to a certain threshold. GCP also provides a $300 free credit when signing up on GCP for the first time.

GCP is organized around *projects*. To access GCP resources and services, we need to create or join a project with our google account, and then navigate from the project dashboard.

Next, we will start setting up and create a GCP project.

</br>

## GCP setup

To do the initial setup, follow the steps below:
1. Create a google account (if you don't have already), then access [Google Cloud Platform](https://console.cloud.google.com).

2. If you are new to GCP, there should be a notification above mentioning $300 GCP free credits to activate, click activate then follow the process.

3. Setup a new project and note down the Project ID.
    1. From the GCP dashboard, click on the drop down menu next to the *Google Cloud Platform* title to show the project list and click on *New project*.
    2. Give the project a name, in my case I used `data-engineering`. The Project ID is autogenerated and must be unique to all of GCP, not just your account. Leave the organization field as *No organization*. Click *Create*.
    3. Back on the dashboard, make sure the newly created project is selected.

4. Setup a service account for this project and download the authentication key.
    1. *IAM & Admin > Service accounts > Create service account*
    2. Give the service account a name. In my case I used `data-engineering-user`. Leave the rest on default, then click *Create and continue*.
    3. Grant the Viewer role (*Basic > Viewer*) then *Continue*.
    4. There is no need to grant users access atm so just click on *Done*.
    5. Once the service account is created, click on the 3 dots on the side, and select *Manage keys*.
    6. *Add key > Create new key*. Select __JSON__ and click *Create*. This will download the authentication key as a JSON file on your computer. Save it wherever and note down the path. In my case, I just put it in this course folder.
5. Download [GCP Software Development Kit](https://cloud.google.com/sdk/docs/quickstart). Follow the instructions to install and connect it to the account and project depending on your machine.
6. Set the environment variable to point to the auth keys.
    1. The method varies depending on the system, in bash the command is:

        ```bash
        export GOOGLE_APPLICATION_CREDENTIALS="<path-to-authkey>.json"
        ```

    2. Refresh the token and verify the authentication with the GCP SDK:

        ```
        gcloud auth application-default login
        ```

Now we are ready to use GCP.

</br>

## GCP access setup

In the following, we will enable some API and add extra credentials for our service account to be able to use some of the services we are going to use in the future.

1. Assign Storage Admin, Storage Object Admin, and BigQuery Admin IAM Roles to the service account from *IAM & Admin > IAM*.
    1. Select the service account we created before and edit the permissions by clicking on the pencil icon.
    2. Add the roles mentioned above and click on *Save*.
        * `Storage Admin`: for creating and managing Google Cloud Storage *buckets*.
        * `Storage Object Admin`: for creating and managing *objects* within the buckets.
        * `Bigquery Admin`: for managing BigQuery resources and data.

2. Enable APIs for the project (we will use these API so that Terraform can interact with GCP):
    * [Identity and Access Management (IAM) API](https://console.cloud.google.com/apis/library/iam.googleapis.com)
    * [IAM Service Account Credentials API](https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com)

3. Make sure that the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set.
    * In bash you can try
        ```bash
        echo $GOOGLE_APPLICATION_CREDENTIALS
        ```

</br>

# Terraform

[Terraform](https://www.terraform.io/) is an open-source tool by HashiCorp that is used to provision infrastructure resource. This tool is called Infrastructure as Code since it let us build, change, and manage our infrastructure in a safe and consistent way by defining resource configuration as code, making it possible to take advantage of tools such as version control. It also allows us to bypass the cloud vendor GUIs.

</br>

## Terraform setup

Installation manual is available [here](https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/gcp-get-started), but I will expand more on the windows 10 version here.

1. Download Terraform [here](https://www.terraform.io/downloads). Make sure the correct operating system for is selected, then download the appropriate package as a zip archive.
2. Extract the zip archive anywhere, this will result in a single binary file `terraform`. Note down the path to this file.
3. Add the path to PATH environment variable.
    1. On your windows search bar, look up *"environment variables"*, this will open up *System Properties* window.
        * Alternatively, *Settings > About > Advanced system settings*
    2. On *System Properties* window, under the *Advanced* tab, click *Environment Variables*.
    3. Under the *User variables for user*, double click on *Path*.
    4. Click *New* then add in the path to the `terraform` binary.
4. Restart your terminal, then verify that the installation works by running the following command:

    ```bash
    terraform -help
    ```

    You should get a listing of `terraform` commands and sub-commands

Next we will cover the basics on terraform

</br>

## Terraform basics

There are 2 important components to Terraform: the code (configuration) files and Terraform commands.

The set of files used to describe infrastructure in Terraform is known as a Terraform *__configuration__*. Terraform configuration files end up in `.tf` for files written in Terraform, or `tf.json` for JSON files. A Terraform configuration must be in its own working directory; you cannot have 2 or more separate configurations in the same folder.

Here's a basic `main.tf` file written in Terraform with all of the necessary info to describe basic infrastructure:

```go
terraform {
    required_providers {
        google = {
            source  = "hashicorp/google"
            version = "3.5.0"
        }
    }
}

provider "google" {
    credentials = file("<path-to-authkey>.json")

    project = "<project-id>"
    region  = "us-central1"
    zone    = "us-central1-c"
}

resource "google_compute_network" "vpc_network" {
    name = "terraform-network"
}
```

>* Terraform divides information into __blocks__, which are defined within braces (`{}`), and the end of a statement is defined by linebreak.
>* By convention, arguments with single-line values in the same nesting level have their equal signs (`=`) aligned for easier reading.

Now to explore on the Terraform configuration above, there are 3 main blocks, the `terraform` block, `providers` block, and the `resource` block.

1. Terraform Block.
    * There can only be a single `terraform` block.
    * This block contains Terraform settings, including required providers Terraform will use to provision the infrastructure, defined by the `required_providers` sub-block.
        * We only provided a single provider here which we've called `google`.
            * Each provider needs a `source` in order to install the right plugin. By default the Hashicorp repository is used, in a similar way to Docker images.
            * `hashicorp/google` is short for `registry.terraform.io/hashicorp/google`.
            * Optionally, a provider can have an enforced `version`. If this is not specified, the latest version will be used by default, which could introduce breaking changes in some rare cases.
    * There are other settings in this block that will be encountered later on.

2. Provider Block.
    * There can be multiple `provider` block.
    * This block configures a specific provider defined above. We only use single `provider` block for `google` provider.
    * The content of this block is provider-specific. Here, we used GCP but it may be different for other providers like AWS.
    * Some of the variables seen in this example, such as `credentials` or `zone`, can be provided by other means which will be covered later.

3. Resource Block.
    * There can be multiple `resource` block.
    * This block is the actual component of the infrastructure. In the example above, we only provision a single resource.
    * `resource` blocks have 2 strings before the braces, the resource *__type__* and the resource *__name__*. Together they create the *resource ID* in the shape of `type.name`.
        * The first prefix of the resource type maps to the name of the provider. For example, the resource type `google_compute_network` has the prefix `google` and thus maps to the provider `google`.
        * The resource types are defined in the Terraform [documentation](https://registry.terraform.io/providers/hashicorp/google/latest/docs) and refer to resources offered by cloud providers. 
    * Resource name before the braces is the internal name that is used by Terraform configurations to refer to each resource and has no impact on the actual infrastructure.
    * The contents of a resource block are specific to the resource type. Refer to the [docs](https://registry.terraform.io/browse/providers) to see a list of resource types by provider.
        * For example, the argument `name` inside the resource block here is specific to resource type `google_compute_network`, which is the name that the resource will have __within__ GCP's infrastructure.

Besides these 3 blocks, there are additional available blocks:

* *__Input variables__* block types are useful for customizing aspects of other blocks without altering the other blocks' source code. They are often referred to as simply *variables*. They are passed at runtime.

    ```go
    variable "region" {
        description = "Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations"
        default = "europe-west6"
        type = string
    }
    ```

    * Description:
        * An input variable block starts with the type `variable` followed by a name of our choosing.
        * The block may contain a number of fields. In this example we use the fields `description`, `type` and `default`.
        * `description` contains a simple description for documentation purposes.
        * `type` specifies the accepted value types for the variable
        * If the `default` field is defined, the variable becomes optional because a default value is already provided by this field. Otherwise, a value must be provided when running the Terraform configuration.
        * For additional fields, check the [Terraform docs](https://www.terraform.io/language/values/variables).
    * Variables must be accessed with the keyword `var.` and then the name of the variable.
    * In our `main.tf` file above, we could access this variable inside the `google` provider block with this line:

        ```go
        region = var.region
        ```

* *__Local values__* block types behave more like constants.

    ```go
    locals {
        region  = "us-central1"
        zone    = "us-central1-c"
    }
    ```

    * Description:
        * Local values may be grouped in one or more blocks of type `locals`. Local values are often grouped according to usage.
        * Local values are simpler to declare than input variables because they are only a key-value pair.
    * Local values must be accessed with the word `local` (*mind the lack of `s` at the end!*).
    
        ```go
        region = local.region
        zone = local.zone
        ```

With a configuration ready, you are now ready to create your infrastructure. There are a number of commands that must be followed:
* `terraform init` : initialize your work directory by downloading the necessary providers/plugins.
* `terraform fmt` (optional): formats your configuration files so that the format is consistent.
* `terraform validate` (optional): returns a success message if the configuration is valid and no errors are apparent.
* `terraform plan` :  creates a preview of the changes to be applied against a remote state, allowing you to review the changes before applying them.
* `terraform apply` : applies the changes to the infrastructure.
* `terraform destroy` : removes your stack from the infrastructure.

</br>

## Create GCP resources with Terraform

We will now create a new `main.tf` file as well as an auxiliary `variables.tf` file with all the blocks we will need for our project.

The infrastructure we will need consists of a Cloud Storage Bucket (`google_storage-bucket`) for our *Data Lake* and a BigQuery Dataset (`google_bigquery_dataset`).

In `main.tf` we will configure the `terraform` block as follows:

```go
terraform {
  required_version = ">= 1.0"
  backend "local" {}
  required_providers {
    google = {
      source  = "hashicorp/google"
    }
  }
}
```

* The `required_version` field states the minimum Terraform version to be used.
* The `backend` field states where we'd like to store the *state* of the infrastructure. `local` means that we'll store it locally in our computers. Alternatively, you could store the state online.

The provider will not make use of the `credentials` field because when we set up GCP access we already created a `GOOGLE_APPLICATION_CREDENTIALS` env-var which Terraform can read in order to get our authentication keys.

In the `variables.tf` we will store variables that may change depending on your needs and location. The ones to note are:
* `region` may vary depending on your geographical location; change it according to your needs.
* `BQ_DATASET` has the name of the table for BigQuery. You may leave it as it is or change it t fit your needs.
* `project` is the Project ID of your project in GCP. SInce the ID is unique, it is good practice to have Terraform as for it every time in case the same code is applied on different projects.

You may access `main.tf` [here](main.tf) and `variables.tf` [here](variables.tf). Take a look at them to understand the details of the implementation. Copy them to a new folder within your work directory so that the subfolder only contains the Terraform configuration files. Now run the following commands:

```bash
terraform init
```

This will download the necessary plugins to connect to GCP and download them to `./.terraform`. Now let's plan the infrastructure:

```bash
terraform plan
```

Terraform will ask for your Project ID. Type it and press enter to let Terraform access GCP and figure out what to do. The infrastructure plan will be printed on screen with all the planned changes marked with a `+` sign next to them.

Let's apply the changes:

```bash
terraform apply
```

You will need to confirm this step by typing `yes` when prompted. This will create all the necessary components in the infrastructure an return a `terraform.tfstate` with the current state of the infrastructure.

After you've successfully created the infrastructure, you may destroy it so that it doesn't consume credit unnecessarily:

```bash
terraform destroy
```

Once again, you will have to confirm this step by typing `yes` when prompted. This will remove your complete stack from the cloud, so only use it when you're 100% sure of it.